{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764ed8d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newsgroups.csv']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import os\n",
    "print(os.listdir(\"input\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098849c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the newsgroups dataset from the CSV file\n",
    "newsgroups = pd.read_csv('input/newsgroups.csv')\n",
    "\n",
    "# extract the text content and integer labels\n",
    "text = newsgroups['data']\n",
    "labels = newsgroups['target']\n",
    "\n",
    "# replace NaN values with empty string\n",
    "newsgroups['data'] = newsgroups['data'].fillna('')\n",
    "\n",
    "# define a stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# set stopwords to english\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd5ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', \n",
    "                'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', \n",
    "                'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', \n",
    "                'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', \n",
    "                'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a36782",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18846.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.293166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.562798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target\n",
       "count  18846.000000\n",
       "mean       9.293166\n",
       "std        5.562798\n",
       "min        0.000000\n",
       "25%        5.000000\n",
       "50%        9.000000\n",
       "75%       14.000000\n",
       "max       19.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79a88b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    999\n",
       "15    997\n",
       "8     996\n",
       "9     994\n",
       "11    991\n",
       "7     990\n",
       "13    990\n",
       "5     988\n",
       "14    987\n",
       "2     985\n",
       "12    984\n",
       "3     982\n",
       "6     975\n",
       "1     973\n",
       "4     963\n",
       "17    940\n",
       "16    910\n",
       "0     799\n",
       "18    775\n",
       "19    628\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1f9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, float) and np.isnan(text):\n",
    "        return \"\"\n",
    "text = text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43547ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    if text is not None:\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    if text is not None:\n",
    "        return re.sub('\\[[^]]*\\]', '', text)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "# apply the preprocessing to the text data\n",
    "text = text.apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd81ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "text = text.apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677152f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to lowercase\n",
    "def preprocess_text(text):\n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "    \n",
    "# apply the preprocessing to the text data\n",
    "text = text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "201ae4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "text = text.apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd15240c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"she's\", 'by', \"haven't\", 'who', 'them', \"aren't\", 'to', 'wouldn', 'isn', \"shan't\", 'below', \"hadn't\", 'just', \"you've\", 'my', 'or', \"isn't\", 't', 'more', 'having', 'doing', 'whom', 'didn', 'out', 'which', 'here', 'nor', 'she', 's', 'he', 'yours', 'those', 'itself', 'her', 'once', \"mustn't\", \"wouldn't\", 'for', 'had', 'that', 'during', 'very', 'both', 'you', 'about', 'we', \"didn't\", 'now', 'ma', 'some', 'with', 'when', 'doesn', 'ours', 'shan', 'from', 'before', 'our', 'been', 'it', 'aren', 'don', 'mightn', 'couldn', 'your', 'ourselves', 'as', 're', 'against', 'because', \"shouldn't\", 'further', 'over', 'did', 'and', 'between', 'do', 'all', \"should've\", 'me', 'most', 'have', 'hers', 'into', 'only', 'were', 'above', 'needn', 'haven', \"mightn't\", 'being', 'myself', 'than', 'o', 'does', 'how', \"couldn't\", \"you'll\", 'own', 'while', 'down', 'hadn', 'under', 'these', 'through', 'm', 'each', 'of', 'is', 'am', \"hasn't\", 'after', 'their', 'theirs', 'other', 'y', 'be', 'i', 'no', \"wasn't\", 'this', 'was', \"don't\", 'on', 'in', 'off', 'can', 'hasn', \"that'll\", 'what', 'few', 'his', 'why', 'at', \"weren't\", \"you'd\", 'ain', 'if', 'wasn', 'him', 'should', 'where', 'himself', 'the', 'then', 'shouldn', 'll', 'a', 'an', 'd', 'until', 'yourselves', 'won', \"won't\", 'there', \"needn't\", 'weren', 'yourself', 'too', 'not', \"doesn't\", 'themselves', 'same', 'its', 'up', 'herself', 'they', \"you're\", \"it's\", 'mustn', 'so', 'are', 'again', 'has', 'will', 'any', 'such', 've', 'but'}\n"
     ]
    }
   ],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "text = text.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f36fb991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         \n",
       "1         \n",
       "2         \n",
       "3         \n",
       "4         \n",
       "        ..\n",
       "18841     \n",
       "18842     \n",
       "18843     \n",
       "18844     \n",
       "18845     \n",
       "Name: data, Length: 18846, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64377b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        10\n",
       "1         3\n",
       "2        17\n",
       "3         3\n",
       "4         4\n",
       "         ..\n",
       "18841    13\n",
       "18842    12\n",
       "18843     3\n",
       "18844     1\n",
       "18845     7\n",
       "Name: target, Length: 18846, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15d4904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15076,) (15076,)\n",
      "(3770,) (3770,)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset  \n",
    "#train dataset\n",
    "train_text=newsgroups.data[:15076]\n",
    "train_labels=newsgroups.target[:15076]\n",
    "#test dataset\n",
    "test_text=newsgroups.data[15076:]\n",
    "test_labels=newsgroups.target[15076:]\n",
    "print(train_text.shape,train_labels.shape)\n",
    "print(test_text.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d8277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15076,) (15076,)\n",
      "(3770,) (3770,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing sets\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(text, labels, test_size=0.2, random_state=42)\n",
    "print(train_text.shape,train_labels.shape)\n",
    "print(test_text.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ef7e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized train data\n",
    "norm_train_text=newsgroups.data[:15076]\n",
    "norm_train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2589e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After a tip from Gary Crum (crum@fcom.cc.utah.edu) I got on the Phone\\nwith \"Pontiac Systems\" or \"Pontaic Customer Service\" or whatever, and\\ninquired about a rumoured Production Hold on the Formula Firebird and\\nTrans Am.  BTW, Talking with the dealer I bought the car from got me\\nnowhere.  After being routed to a \"Firebird Specialist\", I was able\\nto confirm that this is in fact the case.\\n\\nAt first, there was some problem with the 3:23 performance axle ratio.\\nShe wouldn\\'t go into any details, so I don\\'t know if there were some\\nshipped that had problems, or if production was held up because they\\nsimply didn\\'t have the proper parts from the supplier.  As I say, she\\nwas pretty vague on that, so if anyone else knows anything about this,\\nfeel free to respond.  Supposedly, this problem is now solved.\\n\\nSecond, there is a definate shortage of parts that is somehow related\\nto the six-speed Manual transmission.  So as of this posting, there is\\na production hold on these cars.  She claimed part of the delay was\\nnot wanting to use inferior quality parts for the car, and therefore\\nhaving to wait for the right high quality parts...  I\\'m not positive\\nthat this applies to the Camaro as well, but I\\'m guessing it would.\\n\\nCan anyone else shed some light on this?\\n\\nChris S.\\n-- \\n--------------------------------------------------------------------------------\\nChris Silvester      | \"Any man capable of getting himself elected President\\nchriss@sam.amgen.com |  should by no means be allowed to do the job\"\\nchriss@netcom.com    |   - Douglas Adams, The Hitchhiker\\'s Guide to the Galaxy'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalized test data\n",
    "norm_test_text=newsgroups.data[15076:]\n",
    "norm_test_text[18845]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "596a26c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (15076, 2512741)\n",
      "BOW_cv_test: (3770, 2512741)\n"
     ]
    }
   ],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "#transformed training data\n",
    "cv_train_text=cv.fit_transform(norm_train_text)\n",
    "#transformed test data\n",
    "cv_test_text=cv.transform(norm_test_text)\n",
    "print('BOW_cv_train:',cv_train_text.shape)\n",
    "print('BOW_cv_test:',cv_test_text.shape)\n",
    "#vocab=cv.get_feature_names()-toget feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a87003e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (15076, 2512741)\n",
      "Tfidf_test: (3770, 2512741)\n"
     ]
    }
   ],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "#transformed training data\n",
    "tv_train_text=tv.fit_transform(norm_train_text)\n",
    "#transformed test data\n",
    "tv_test_text=tv.transform(norm_test_text)\n",
    "print('Tfidf_train:',tv_train_text.shape)\n",
    "print('Tfidf_test:',tv_test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba0c993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 20)\n"
     ]
    }
   ],
   "source": [
    "#labeling the target data\n",
    "lb=LabelBinarizer()\n",
    "#transformed labels data\n",
    "label_data=lb.fit_transform(newsgroups['target'])\n",
    "print(label_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e092c04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n",
      "[[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Splitting the target data\n",
    "train_labels=label_data[:15076]\n",
    "test_labels=label_data[15076:]\n",
    "print(train_labels)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42763ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d4797aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting 2d array to 1d \n",
    "#skipping generates error(1d array is expected in LR)\n",
    "#doing this gets the kernel stuck\n",
    "\n",
    "train_labels = np.argmax(train_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d13056e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "87e4624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model for Bag of words\n",
    "lr_bow=lr.fit(cv_train_text,train_labels)\n",
    "print(lr_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model for tfidf features\n",
    "lr_tfidf=lr.fit(tv_train_text,train_labels)\n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the model for bag of words\n",
    "lr_bow_predict=lr.predict(cv_test_text)\n",
    "print(lr_bow_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b57827",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predicting the model for tfidf features\n",
    "lr_tfidf_predict=lr.predict(tv_test_text)\n",
    "print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Experimental below/Don't run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f931211",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a logistic regression model\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "# train the model on the training data\n",
    "lr_clf.fit(X_train,y_train)\n",
    "\n",
    "# make predictions on the training data\n",
    "lr_pred = lr_clf.predict(X_train)\n",
    "train_score = accuracy_score(y_train, lr_pred) * 100\n",
    "print(f\"Train accuracy score: {train_score:.2f}%\")\n",
    "\n",
    "# make predictions on the testing data\n",
    "lr_pred = lr_clf.predict(cv_test_text)\n",
    "test_score = accuracy_score(test_labels, lr_pred) * 100\n",
    "print(f\"Test accuracy score: {test_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b168c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train.toarray(), y_train)\n",
    "\n",
    "nb_pred = nb_clf.predict(X_train.toarray())\n",
    "train_score = accuracy_score(y_train, nb_pred) * 100\n",
    "print(f\"Train accuracy score: {train_score:.2f}%\")\n",
    "\n",
    "nb_pred = nb_clf.predict(X_test.toarray())\n",
    "test_score = accuracy_score(test_labels, nb_pred) * 100\n",
    "print(f\"Test accuracy score: {test_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97791f67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train.toarray(), y_train)\n",
    "print(\"Training completed\")\n",
    "\n",
    "svm_pred = svm_clf.predict(X_train.toarray())\n",
    "train_score = accuracy_score(y_train, svm_pred) * 100\n",
    "print(f\"Train accuracy score: {train_score:.2f}%\")\n",
    "\n",
    "svm_pred = svm_clf.predict(X_test.toarray())\n",
    "test_score = accuracy_score(y_test, svm_pred) * 100\n",
    "print(f\"Test accuracy score: {test_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506eb59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"/Users/bibeksharma/Downloads/4\")\n",
    "encoder_inputs = preprocessor(text_input)\n",
    "encoder = hub.KerasLayer(\n",
    "    \"/Users/bibeksharma/Downloads/4/\")\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 768]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ce0f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = np.random.random((1000, 100))\n",
    "y_train = np.random.random((1000, 10))\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7894afba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"AVAILABLE\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bea3e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create some tensors to use\n",
    "a = tf.random.normal([1000, 1000])\n",
    "b = tf.random.normal([1000, 1000])\n",
    "\n",
    "# Print the currently used physical device\n",
    "print(\"Currently used device:\", tf.test.gpu_device_name() or \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af3fa34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ec26d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 96.91%\n",
      "Test accuracy score: 77.20%\n"
     ]
    }
   ],
   "source": [
    "# create a logistic regression model\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "# train the model on the training data\n",
    "lr_clf.fit(X_train,y_train)\n",
    "\n",
    "# make predictions on the training data\n",
    "lr_pred = lr_clf.predict(X_train)\n",
    "train_score = accuracy_score(y_train, lr_pred) * 100\n",
    "print(f\"Train accuracy score: {train_score:.2f}%\")\n",
    "\n",
    "# make predictions on the testing data\n",
    "lr_pred = lr_clf.predict(cv_test_text)\n",
    "test_score = accuracy_score(test_labels, lr_pred) * 100\n",
    "print(f\"Test accuracy score: {test_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df1a23e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 92.15%\n",
      "Test accuracy score: 74.20%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train.toarray(), y_train)\n",
    "\n",
    "nb_pred = nb_clf.predict(X_train.toarray())\n",
    "train_score = accuracy_score(y_train, nb_pred) * 100\n",
    "print(f\"Train accuracy score: {train_score:.2f}%\")\n",
    "\n",
    "nb_pred = nb_clf.predict(X_test.toarray())\n",
    "test_score = accuracy_score(test_labels, nb_pred) * 100\n",
    "print(f\"Test accuracy score: {test_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d7f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating Model ... \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Evaluating Model ... \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_classes(X_test_Glove)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mclassification_report(y_test, predicted))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n Evaluating Model ... \\n\")\n",
    "predicted = model.predict_classes(X_test_Glove)\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(\"\\n\")\n",
    "logger = logging.getLogger(\"logger\")\n",
    "result = compute_metrics(y_test, predicted)\n",
    "for key in (result.keys()):\n",
    "    logger.info(\"  %s = %s\", key, str(result[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736ce161",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X_train_Glove, y_train,\n\u001b[1;32m      2\u001b[0m                            validation_data\u001b[38;5;241m=\u001b[39m(X_test_Glove,y_test),\n\u001b[1;32m      3\u001b[0m                            epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      4\u001b[0m                            batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m      5\u001b[0m                            verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_Glove, y_train,\n",
    "                           validation_data=(X_test_Glove,y_test),\n",
    "                           epochs=5,\n",
    "                           batch_size=128,\n",
    "                           verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbea4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
